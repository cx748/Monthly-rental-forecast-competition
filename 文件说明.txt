train.csv――原始训练集
test.csv――原始测试集

train_data.pickle:80%构成的训练集 train_y.pickle:80%构成的训练集的y val_data.pickle:20%构成的验证集 val_y.pickle:20%构成的验证集的y
test_data.pickle:所有的测试集 train_data_all.pickle:100%的训练集 train_y_all.pickle:100%的训练集的y

t1.py――查看原始训练集的各字段的信息

train1.csv――对原始数据集train.csv进行预处理，删除了第一行的字段信息
	第一行各字段：
		时间	小区名	小区房屋出租数量	楼层	总楼层	房屋面积	房屋朝向	卧室数量	厅的数量	卫的数量	区	位置	地铁线路	地铁站点	距离	月租金
	对“小区房屋出租数量”字段中空的值全部取字段的平均值0.122320668（该方案可优化）
	删除字段“居住状态”、“出租方式”、“装修情况”
	删除“区”和“位置”为空的31条数据
	但“地铁线路”、“地铁站店”、“距离”仍有空值。――由程序处理

test1.csv――类似地对原始测试集test1.csv进行如上预处理
	对于“区”为空的数据，取训练集中的众数12作为其数值
	对于“位置”为空的数据，取训练集中的众数52作为其数值
	对于“时间”，等分成1、2、3
	另外，删除第一列id
	test1b.csv为test1.csv的优化版

t3.py――将train1.csv中的房屋朝向进行划分，划分成8个label（8个字段），并且把结果输出到train_1_directions.csv
	0-7：东 南 西 北 东南 东北 西南 西北
	由输出的train_1_directions.csv和train1.csv相结合来构成train2.csv

t3_t.py――针对test1.csv的房屋朝向进行划分，把结果输出到test_1_directions.csv

train2.csv――执行t3.py后，将房屋朝向转化为了8个字段（东 南 西 北 东南 东北 西南 西北）
	第一行各字段：	
		时间	小区名	小区房屋出租数量	楼层	总楼层	房屋面积	房屋朝向#1		房屋朝向#2		 房屋朝向#3 	房屋朝向#4		房屋朝向#5		房屋朝向#6		房屋朝向#7		房屋朝向#8		卧室数量	厅的数量	卫的数量	区	位置	地铁线路	地铁站点	距离	月租金
	说明：该数据集中，S列下缺少数值76，U列下缺少数值60，P列下缺少数值7。为了one-hot编码后能和测试集一致，在最后添加一条假数据
		
test2.csv――执行t3_t.py后，将房屋朝向转化为了8个字段（东 南 西 北 东南 东北 西南 西北）
	人为在最后填补4条数据，以弥补one-hot编码后训练集与测试集字段数量的不一致。
	最终训练完成，预测出结果后，应当删除这些数据。
	test2b.csv为test2.csv的优化版
		
t2.py――（作为代码样版，可以进行后续操作）查看训练集各字段，完成缺失值填充，绘制月租金分布图
	读取train2.csv的数据
	对“地铁线路”为空的字段的值全部取0，对应的“地铁站店”全部取0，对应的“距离”全部取1
	删除“小区名”字段
	
t4.py――t2.py的拷贝版，对连续变量进行相关性计算，对分类字段进行one-hot编码，并且分割训练集与验证集
	以下述方式确定各字段类型：
		时间――分类变量，取值范围1-3，进行one-hot编码
		小区房屋出租数量――连续变量，取值范围0-1
		楼层――分类变量，取值范围0-2，进行one-hot编码
		总楼层――连续变量，取值范围0-1
		房屋面积――连续变量，取值范围0-1
		房屋朝向――分类变量，已经在t3.py中完成了one-hot编码，分成了8个字段
		卧室数量――连续变量，取值范围0-11
		厅的数量――连续变量，取值范围0-8
		卫的数量――连续变量，取值范围0-8
		区――分类变量，取值范围0-14，进行one-hot编码
		位置――分类变量，取值范围0-152，进行one-hot编码
		地铁线路――分类变量，取值范围0-5，进行one-hot编码
		地铁站点――分类变量，取值范围0-119，进行one-hot编码
		距离――连续变量，取值范围0-1
		月租金――连续变量，预测目标
	分割完成后将训练数据和标签与验证数据和标签保存到.pickle文件

t4_t.py――针对测试集进行t4.py中的操作，对分类字段进行one-hot编码
	将测试数据保存到.pickle文件

t5.py――读取t4.py中导出的数据集与验证集，使用随机森林进行特征选择
	如果选择所有自变量进行多元线性回归，RMSE=4.2963
	使用随机森林进行特征提取，将特征重要性导出到feature_importance_using_randomForest.csv中
	（特征选择可优化）
	过滤掉重要性小于0.001的所有特征，将filter_feature导出至filter_feature.pickle

t6.py――读取上述filter_feature，使用岭回归、决策树回归、KNN和随机森林回归、Adaboost、GBRT进行训练，比较模型优劣
	对过滤后的特征进行建模和训练：
		使用岭回归：RMSE=4.4411
		使用SVM回归：RMSE=4.5122
		使用KNN回归：RMSE=3.3590
		使用决策树回归：RMSE=1.8058
		使用随机森林回归：RMSE=1.5227
		调节超参数的随机森林回归：最优参数：{'max_depth': 50, 'max_features': 'auto', 'n_estimators': 40}
		使用Adaboost回归：
			200棵树，学习率0.001，基于决策树：RMSE=1.3974
		使用XGBRegressor回归：
			150棵树，学习率0.1，最大深度50,：RMSE=1.4089
			200棵树，学习率0.1，最大深度100：RMSE=1.4192
			200棵树，学习率0.05，最大深度100：RMSE=1.4110
			300棵树，学习率0.1，最大深度100：RMSE=1.4198
			300棵树，学习率0.2，最大深度20：RMSE=1.3886
			400棵树，学习率0.1，最大深度20：RMSE=1.3949
			

t6_test.py――分别针对不同数量的特征，选择最优的特征组合
	取5棵树：
		61个特征――RMSE=1.62
		前10特征――RMSE=2.35
		前15特征――RMSE=2.13
		前20特征――RMSE=1.83
		前30特征――RMSE=1.71
	取100棵树，最大深度为20：
		61个特征――RMSE=1.42
		100个特征――RMSE=1.3935
		150个特征，最大深度为40――RMSE=1.3594
		200个特征，最大深度为60――RMSE=1.3309
		
		
t7.py――针对t6.py，对真实的测试集进行预测（使用{50,auto,40}的随机森林回归）
	将结果导出至predict_result.txt（记得把最后的4个数值删除）

t8.py――p7.py的模型集成版，大量地做随机森林回归，取平均值
	平均预测结果导出至avg_predict_result.txt
	所有预测结果导出至all_predict_result.csv

t8_b.py――模型集成，使用Adaboost回归来实现

t8_c.py――模型集成，使用XGBoost回归来实现

	